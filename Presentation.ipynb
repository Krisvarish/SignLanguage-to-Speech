{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f07659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import time\n",
    "\n",
    "# Load models with error handling\n",
    "def load_asl_model(model_path):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(model_path, compile=False)\n",
    "        return model\n",
    "    except ValueError as e:\n",
    "        print(\"Error loading model:\", e)\n",
    "        return None\n",
    "\n",
    "asl_model = load_asl_model('asl_cnn_model.h5')\n",
    "emotion_model = load_asl_model('fer2013_emotion_model.h5')\n",
    "\n",
    "# Define labels\n",
    "asl_labels = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n",
    "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "\n",
    "# Initialize pyttsx3 for speech\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face_img):\n",
    "    face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "    face_img = cv2.resize(face_img, (48, 48))\n",
    "    face_img = face_img.astype(\"float32\") / 255.0\n",
    "    face_img = np.expand_dims(face_img, axis=-1)\n",
    "    face_img = np.expand_dims(face_img, axis=0)\n",
    "    emotion_prediction = emotion_model.predict(face_img)\n",
    "    emotion_index = np.argmax(emotion_prediction)\n",
    "    return emotion_labels[emotion_index]\n",
    "\n",
    "# Function to predict ASL\n",
    "def predict_asl(sign_img):\n",
    "    sign_img = cv2.resize(sign_img, (64, 64))\n",
    "    sign_img = sign_img.astype('float32') / 255.0\n",
    "    sign_img = np.expand_dims(sign_img, axis=-1)\n",
    "    sign_img = np.expand_dims(sign_img, axis=0)\n",
    "    asl_prediction = asl_model.predict(sign_img)\n",
    "    asl_index = np.argmax(asl_prediction)\n",
    "    return asl_labels[asl_index]\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Webcam Input\", frame)\n",
    "\n",
    "    # Detect face and predict emotion\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        face_img = frame[y:y + h, x:x + w]\n",
    "        emotion = predict_emotion(face_img)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Predict ASL\n",
    "    asl_sign = predict_asl(frame)\n",
    "    cv2.putText(frame, f\"ASL: {asl_sign}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "    # Text to speech\n",
    "    engine.say(f\"Sign Detected: {asl_sign}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "    cv2.imshow(\"ASL and Emotion Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6152a96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-pptx"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\venka\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from python-pptx) (4.8.0)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from python-pptx) (9.0.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from python-pptx) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\venka\\anaconda3\\lib\\site-packages (from python-pptx) (4.12.2)\n",
      "Installing collected packages: python-pptx\n",
      "Successfully installed python-pptx-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-pptx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1141746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "# Create a presentation object\n",
    "prs = Presentation()\n",
    "\n",
    "# Slide 1: Title Slide\n",
    "slide_1 = prs.slides.add_slide(prs.slide_layouts[0])\n",
    "title = slide_1.shapes.title\n",
    "subtitle = slide_1.placeholders[1]\n",
    "\n",
    "title.text = \"Real-Time American Sign Language to Speech System using Deep Learning\"\n",
    "subtitle.text = \"Integrating Sign Recognition and Emotion-Aware Speech Synthesis\\nPresented by: [Your Name]\\nInstitution: [Your Institution Name]\\nCourse: Design and Analysis of Algorithms\\nDate: [Insert Date]\"\n",
    "\n",
    "# Styling title\n",
    "title.text_frame.paragraphs[0].font.size = Pt(18)\n",
    "subtitle.text_frame.paragraphs[0].font.size = Pt(18)\n",
    "subtitle.text_frame.paragraphs[0].font.bold = True\n",
    "subtitle.text_frame.paragraphs[0].font.color.rgb = RGBColor(0, 102, 204)\n",
    "\n",
    "# Slide 2: Introduction\n",
    "slide_2 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_2.shapes.title\n",
    "content = slide_2.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Introduction\"\n",
    "content.text = (\n",
    "    \"Communication is a basic human need, but individuals who are deaf or speech-impaired often struggle due to lack of accessible communication tools.\\n\"\n",
    "    \"Sign Language is a rich and expressive visual language.\\n\"\n",
    "    \"This project bridges communication by recognizing ASL signs and converting them into emotionally contextualized speech using AI.\\n\"\n",
    "    \"Emphasis on real-time interaction and accurate prediction through deep learning.\"\n",
    ")\n",
    "\n",
    "# Slide 3: Problem Statement\n",
    "slide_3 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_3.shapes.title\n",
    "content = slide_3.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Problem Statement\"\n",
    "content.text = (\n",
    "    \"Challenge: Bridging the communication barrier between hearing-impaired and hearing individuals.\\n\\n\"\n",
    "    \"Gap in Current Systems:\\n\"\n",
    "    \"- Many systems are expensive, require gloves or external sensors.\\n\"\n",
    "    \"- Lack of emotion awareness.\\n\"\n",
    "    \"- Not hardware-friendly or deployable.\\n\\n\"\n",
    "    \"Goal: Develop a webcam-based, real-time, emotion-aware ASL-to-speech system using CNN.\"\n",
    ")\n",
    "\n",
    "# Slide 4: Objectives\n",
    "slide_4 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_4.shapes.title\n",
    "content = slide_4.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Objectives\"\n",
    "content.text = (\n",
    "    \"1. Train deep learning models to recognize static ASL alphabet signs.\\n\"\n",
    "    \"2. Recognize facial expressions to capture emotional tone.\\n\"\n",
    "    \"3. Generate natural, expressive speech from sign input.\\n\"\n",
    "    \"4. Deploy the system using a simple camera without extra hardware.\"\n",
    ")\n",
    "\n",
    "# Slide 5: Sustainable Development Goals (SDGs)\n",
    "slide_5 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_5.shapes.title\n",
    "content = slide_5.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Sustainable Development Goals (SDGs)\"\n",
    "content.text = (\n",
    "    \"Goal 4: Quality Education – Provides educational tools for inclusive communication.\\n\"\n",
    "    \"Goal 10: Reduced Inequality – Promotes digital inclusion for hearing-impaired individuals.\\n\"\n",
    "    \"Goal 9: Industry, Innovation and Infrastructure – Utilizes AI in developing accessible tools.\"\n",
    ")\n",
    "\n",
    "# Slide 6: Dataset Overview\n",
    "slide_6 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_6.shapes.title\n",
    "content = slide_6.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Dataset Overview\"\n",
    "content.text = (\n",
    "    \"ASL Dataset:\\n\"\n",
    "    \"- 29 Classes (A–Z, 'del', 'nothing', 'space')\\n\"\n",
    "    \"- ~87,000 images, RGB, 200x200 px\\n\\n\"\n",
    "    \"FER2013 Emotion Dataset:\\n\"\n",
    "    \"- 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\\n\"\n",
    "    \"- Grayscale, 48x48 px, ~35,000 images\"\n",
    ")\n",
    "\n",
    "# Slide 7: Preprocessing Steps\n",
    "slide_7 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_7.shapes.title\n",
    "content = slide_7.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Preprocessing Steps\"\n",
    "content.text = (\n",
    "    \"1. Resize and normalize images\\n\"\n",
    "    \"2. Label encoding for ASL letters and emotions\\n\"\n",
    "    \"3. One-hot encoding of labels\\n\"\n",
    "    \"4. Augmentation: Random flip, rotation, shift (optional)\\n\"\n",
    "    \"5. Split: 80% training, 20% validation\"\n",
    ")\n",
    "\n",
    "# Slide 8: CNN Architecture for ASL Recognition\n",
    "slide_8 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_8.shapes.title\n",
    "content = slide_8.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"CNN Architecture for ASL Recognition\"\n",
    "content.text = (\n",
    "    \"Input: 200x200x3 image\\n\\n\"\n",
    "    \"3 Convolutional Blocks:\\n\"\n",
    "    \"- Conv2D → ReLU → MaxPooling\\n\\n\"\n",
    "    \"Flatten Layer\\n\"\n",
    "    \"Dense(128) → Dropout(0.5)\\n\"\n",
    "    \"Output Layer: 29 neurons (softmax)\\n\\n\"\n",
    "    \"Accuracy: ~98%\"\n",
    ")\n",
    "\n",
    "# Slide 9: CNN Architecture for Emotion Detection\n",
    "slide_9 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_9.shapes.title\n",
    "content = slide_9.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"CNN Architecture for Emotion Detection\"\n",
    "content.text = (\n",
    "    \"Input: 48x48 grayscale face image\\n\\n\"\n",
    "    \"Conv2D (3 layers): ReLU + MaxPooling\\n\\n\"\n",
    "    \"Flatten → Dense(128) → Dropout\\n\"\n",
    "    \"Output: 7-class softmax\\n\\n\"\n",
    "    \"Accuracy: ~89%\"\n",
    ")\n",
    "\n",
    "# Slide 10: System Flow (Architecture Diagram)\n",
    "slide_10 = prs.slides.add_slide(prs.slide_layouts[5])\n",
    "title = slide_10.shapes.title\n",
    "title.text = \"System Flow (Architecture Diagram)\"\n",
    "# Insert the actual diagram as an image\n",
    "slide_10.shapes.add_picture('sign_to_speech_archi.png', Inches(0.5), Inches(1.5), width=Inches(9))\n",
    "\n",
    "# Slide 11: Real-Time Pipeline Details\n",
    "slide_11 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_11.shapes.title\n",
    "content = slide_11.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Real-Time Pipeline Details\"\n",
    "content.text = (\n",
    "    \"1. Open webcam\\n\"\n",
    "    \"2. Define hand and face regions\\n\"\n",
    "    \"3. Predict character every 10 frames\\n\"\n",
    "    \"4. Maintain sliding buffer for prediction stability\\n\"\n",
    "    \"5. Capture sentence\\n\"\n",
    "    \"6. Detect emotion\\n\"\n",
    "    \"7. Generate speech with appropriate tone and volume\"\n",
    ")\n",
    "\n",
    "# Slide 14: Experimental Setup\n",
    "slide_14 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_14.shapes.title\n",
    "content = slide_14.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Experimental Setup\"\n",
    "content.text = (\n",
    "    \"Hardware: i5 Processor, 8GB RAM, No GPU\\n\"\n",
    "    \"Environment: Jupyter Notebook\\n\\n\"\n",
    "    \"Models:\\n\"\n",
    "    \"- ASL CNN Model (.keras)\\n\"\n",
    "    \"- FER2013 CNN Model (.keras)\\n\\n\"\n",
    "    \"Accuracy:\\n\"\n",
    "    \"- ASL Model: ~98%\\n\"\n",
    "    \"- Emotion Model: ~89%\\n\\n\"\n",
    "    \"Real-time latency: ~150ms/frame\"\n",
    ")\n",
    "\n",
    "# Slide 15: Existing Solutions vs Our Model\n",
    "slide_15 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_15.shapes.title\n",
    "content = slide_15.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Existing Solutions vs Our Model\"\n",
    "content.text = (\n",
    "    \"Feature        | SignAll | DeepASL | Smart Gloves | Proposed System\\n\"\n",
    "    \"---------------------------------------------------------\\n\"\n",
    "    \"Hardware-Free  | No      | No      | No           | Yes\\n\"\n",
    "    \"Emotion Detection| No    | No      | No           | Yes\\n\"\n",
    "    \"Real-Time      | Yes     | No      | Yes          | Yes\\n\"\n",
    "    \"Cost-effective | No      | No      | No           | Yes\\n\"\n",
    "    \"Open Source    | No      | Yes     | No           | Yes\"\n",
    ")\n",
    "\n",
    "# Slide 16: Key Challenges\n",
    "slide_16 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_16.shapes.title\n",
    "content = slide_16.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Key Challenges\"\n",
    "content.text = (\n",
    "    \"1. Visual similarity between some signs (e.g., M vs N)\\n\"\n",
    "    \"2. Lighting and background noise\\n\"\n",
    "    \"3. Misclassification due to hand misplacement\\n\"\n",
    "    \"4. Real-time performance on low-end hardware\\n\"\n",
    "    \"5. Integrating emotional tone to speech output\"\n",
    ")\n",
    "\n",
    "# Slide 17: Future Scope\n",
    "slide_17 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_17.shapes.title\n",
    "content = slide_17.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Future Scope\"\n",
    "content.text = (\n",
    "    \"1. Use hand landmark models like MediaPipe\\n\"\n",
    "    \"2. Extend support for dynamic gestures\\n\"\n",
    "    \"3. Add full sentence and grammar construction\\n\"\n",
    "    \"4. Deploy as mobile/web app\\n\"\n",
    "    \"5. Add multilingual speech output\"\n",
    ")\n",
    "\n",
    "# Slide 18: Conclusion\n",
    "slide_18 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_18.shapes.title\n",
    "content = slide_18.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"Conclusion\"\n",
    "content.text = (\n",
    "    \"1. Successfully created a novel, real-time ASL-to-speech system\\n\"\n",
    "    \"2. Incorporated emotion detection to enhance expressiveness\\n\"\n",
    "    \"3. Outperforms many existing solutions in affordability and scope\\n\"\n",
    "    \"4. Promotes inclusion and accessibility using AI\"\n",
    ")\n",
    "\n",
    "# Slide 19: References\n",
    "slide_19 = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide_19.shapes.title\n",
    "content = slide_19.shapes.placeholders[1]\n",
    "\n",
    "title.text = \"References\"\n",
    "content.text = (\n",
    "    \"1. Kaggle ASL Alphabet Dataset\\n\"\n",
    "    \"2. FER2013 Emotion Dataset\\n\"\n",
    "    \"3. TensorFlow/Keras Documentation\\n\"\n",
    "    \"4. OpenCV Python Docs\\n\"\n",
    "    \"5. Pyttsx3 TTS Engine\\n\"\n",
    "    \"6. Google Mediapipe\"\n",
    ")\n",
    "\n",
    "# Save presentation\n",
    "prs.save(\"ASL_to_Speech_Full_Presentation.pptx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f88c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
