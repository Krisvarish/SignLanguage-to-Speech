{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# Paths\n",
    "train_dir = 'ASL_dataset/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "# Image properties\n",
    "img_width, img_height = 64, 64\n",
    "batch_size = 32\n",
    "\n",
    "# Data generator with validation split\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def clean_dataset(folder):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.verify()  # Verify image integrity\n",
    "            except (UnidentifiedImageError, OSError):\n",
    "                print(f\"Deleting invalid image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "\n",
    "# Use it on your training folder\n",
    "clean_dataset('ASL_dataset/asl_alphabet_train/asl_alphabet_train')\n",
    "\n",
    "\n",
    "# Training data (80%)\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation data (20%)\n",
    "val_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"asl_cnn_model2.keras\", save_format=\"keras_v3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fe529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"fer2013.csv\")\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_data(df):\n",
    "    pixels = df['pixels'].tolist()\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split()]\n",
    "        face = np.asarray(face).reshape(48, 48)\n",
    "        faces.append(face)\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    faces = faces / 255.0  # normalize\n",
    "    emotions = to_categorical(df['emotion'], num_classes=7)\n",
    "    return faces, emotions\n",
    "\n",
    "# Split data\n",
    "train_df = df[df['Usage'] == 'Training']\n",
    "test_df = df[df['Usage'] == 'PublicTest']  # or 'PrivateTest'\n",
    "\n",
    "x_train, y_train = preprocess_data(train_df)\n",
    "x_test, y_test = preprocess_data(test_df)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')  # 7 emotion classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"fer2013_emotion_model2.keras\", save_format=\"keras_v3\")\n",
    "print(\"Model saved as fer2013_emotion_model.keras\")\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "asl_model = load_model(\"asl_cnn_model.keras\")\n",
    "emotion_model = load_model(\"fer2013_emotion_model.keras\")\n",
    "\n",
    "# Load label maps\n",
    "asl_labels = sorted(list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))  # Assuming 26 classes\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set up webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "sentence = \"\"\n",
    "frame_count = 0\n",
    "predicted_char = \"\"\n",
    "\n",
    "def get_emotion(face_img_gray):\n",
    "    resized = cv2.resize(face_img_gray, (48, 48))\n",
    "    resized = resized.reshape(1, 48, 48, 1) / 255.0\n",
    "    prediction = emotion_model.predict(resized, verbose=0)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "def get_sign_language_char(hand_img_color):\n",
    "    resized = cv2.resize(hand_img_color, (64, 64))\n",
    "    resized = resized.reshape(1, 64, 64, 3) / 255.0\n",
    "    prediction = asl_model.predict(resized, verbose=0)\n",
    "    return asl_labels[np.argmax(prediction)]\n",
    "\n",
    "def speak_text(text, emotion):\n",
    "    if emotion == \"Sad\":\n",
    "        engine.setProperty(\"rate\", 130)\n",
    "        engine.setProperty(\"volume\", 0.6)\n",
    "    elif emotion == \"Happy\":\n",
    "        engine.setProperty(\"rate\", 180)\n",
    "        engine.setProperty(\"volume\", 1.0)\n",
    "    elif emotion == \"Angry\":\n",
    "        engine.setProperty(\"rate\", 160)\n",
    "        engine.setProperty(\"volume\", 1.0)\n",
    "    else:\n",
    "        engine.setProperty(\"rate\", 150)\n",
    "        engine.setProperty(\"volume\", 0.9)\n",
    "\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "print(\"Press 's' to save predicted character to sentence\")\n",
    "print(\"Press 't' to speak the sentence\")\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip for natural interaction\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Define regions\n",
    "    hand_region = frame[100:300, 400:600]\n",
    "    face_region = frame[50:250, 50:250]\n",
    "    face_gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Predict sign every 30 frames\n",
    "    if frame_count % 30 == 0:\n",
    "        predicted_char = get_sign_language_char(hand_region)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Display hand and face regions\n",
    "    cv2.rectangle(frame, (400, 100), (600, 300), (255, 0, 0), 2)\n",
    "    cv2.rectangle(frame, (50, 50), (250, 250), (0, 255, 0), 2)\n",
    "\n",
    "    # Display current predictions\n",
    "    cv2.putText(frame, f\"Predicted Sign: {predicted_char}\", (20, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"Sentence: {sentence}\", (20, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 255, 50), 2)\n",
    "\n",
    "    cv2.imshow(\"Sign Language to Speech\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s'):\n",
    "        sentence += predicted_char\n",
    "    elif key == ord('t'):\n",
    "        emotion = get_emotion(face_gray)\n",
    "        print(f\"Detected emotion: {emotion}\")\n",
    "        speak_text(sentence, emotion)\n",
    "        sentence = \"\"\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ac2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce70791",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ac315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create dummy data\n",
    "x = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = np.array([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Build simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=[1])\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "model.fit(x, y, epochs=5)\n",
    "\n",
    "# Predict\n",
    "print(model.predict([[5.0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models\n",
    "asl_model = load_model(\"asl_cnn_model.keras\")\n",
    "emotion_model = load_model(\"fer2013_emotion_model.keras\")\n",
    "\n",
    "# Correct label mapping based on dataset\n",
    "asl_labels = ['A', 'B', 'C', 'D', 'del', 'E', 'F', 'G', 'H', 'I',\n",
    "              'J', 'K', 'L', 'M', 'N', 'nothing', 'O', 'P', 'Q',\n",
    "              'R', 'S', 'space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables\n",
    "sentence = \"\"\n",
    "stable_char = \"\"\n",
    "live_prediction = \"\"\n",
    "frame_count = 0\n",
    "last_pred = \"\"\n",
    "same_count = 0\n",
    "stable_threshold = 15\n",
    "\n",
    "def get_emotion(face_img_gray):\n",
    "    resized = cv2.resize(face_img_gray, (48, 48))\n",
    "    resized = resized.reshape(1, 48, 48, 1) / 255.0\n",
    "    prediction = emotion_model.predict(resized, verbose=0)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "def get_sign_language_char(hand_img_color):\n",
    "    resized = cv2.resize(hand_img_color, (64, 64))\n",
    "    resized = resized.reshape(1, 64, 64, 3) / 255.0\n",
    "    prediction = asl_model.predict(resized, verbose=0)\n",
    "    confidence = np.max(prediction)\n",
    "    label_index = np.argmax(prediction)\n",
    "\n",
    "    if label_index >= len(asl_labels):\n",
    "        return \"\", 0.0\n",
    "    return asl_labels[label_index], confidence\n",
    "\n",
    "def speak_text(text, emotion):\n",
    "    if emotion == \"Sad\":\n",
    "        engine.setProperty(\"rate\", 130)\n",
    "        engine.setProperty(\"volume\", 0.6)\n",
    "    elif emotion == \"Happy\":\n",
    "        engine.setProperty(\"rate\", 180)\n",
    "        engine.setProperty(\"volume\", 1.0)\n",
    "    elif emotion == \"Angry\":\n",
    "        engine.setProperty(\"rate\", 160)\n",
    "        engine.setProperty(\"volume\", 1.0)\n",
    "    else:\n",
    "        engine.setProperty(\"rate\", 150)\n",
    "        engine.setProperty(\"volume\", 0.9)\n",
    "\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "print(\"Press 's' to save predicted character to sentence\")\n",
    "print(\"Press 't' to speak the sentence\")\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    hand_region = frame[100:300, 400:600]\n",
    "    face_region = frame[50:250, 50:250]\n",
    "    face_gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Predict every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        label, conf = get_sign_language_char(hand_region)\n",
    "        live_prediction = label  # Always show current prediction live\n",
    "\n",
    "        if label == last_pred:\n",
    "            same_count += 1\n",
    "        else:\n",
    "            same_count = 0\n",
    "        last_pred = label\n",
    "\n",
    "        if same_count >= stable_threshold:\n",
    "            stable_char = label\n",
    "            same_count = 0\n",
    "\n",
    "        print(f\"[Live] {label} (Conf: {conf:.2f})\")\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Draw ROIs\n",
    "    cv2.rectangle(frame, (400, 100), (600, 300), (255, 0, 0), 2)\n",
    "    cv2.rectangle(frame, (50, 50), (250, 250), (0, 255, 0), 2)\n",
    "\n",
    "    # Display live prediction and sentence\n",
    "    cv2.putText(frame, f\"Live Sign: {live_prediction}\", (20, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"Sentence: {sentence}\", (20, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 255, 50), 2)\n",
    "\n",
    "    cv2.imshow(\"Sign Language to Speech\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s') and stable_char != \"\":\n",
    "        if stable_char == \"space\":\n",
    "            sentence += \" \"\n",
    "        elif stable_char == \"del\" and len(sentence) > 0:\n",
    "            sentence = sentence[:-1]\n",
    "        elif stable_char not in [\"nothing\", \"space\", \"del\"]:\n",
    "            sentence += stable_char\n",
    "        print(f\"Updated sentence: {sentence}\")\n",
    "    elif key == ord('t'):\n",
    "        emotion = get_emotion(face_gray)\n",
    "        print(f\"Detected emotion: {emotion}\")\n",
    "        speak_text(sentence, emotion)\n",
    "        sentence = \"\"\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asl_labels = sorted(list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "print(asl_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9145d3c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (280513042.py, line 111)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    sentence += \" \"qq\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models\n",
    "asl_model = load_model(\"asl_cnn_model.keras\")\n",
    "emotion_model = load_model(\"fer2013_emotion_model.keras\")\n",
    "\n",
    "asl_labels = ['A', 'B', 'C', 'D', 'del', 'E', 'F', 'G', 'H', 'I',\n",
    "              'J', 'K', 'L', 'M', 'N', 'nothing', 'O', 'P', 'Q',\n",
    "              'R', 'S', 'space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables\n",
    "sentence = \"\"\n",
    "live_prediction = \"\"\n",
    "stable_char = \"\"\n",
    "last_pred = \"\"\n",
    "same_count = 0\n",
    "frame_count = 0\n",
    "cooldown = 0\n",
    "\n",
    "stable_threshold = 20  # How many same predictions to stabilize\n",
    "predict_interval = 15  # Predict every N frames\n",
    "cooldown_frames = 20   # Cooldown after saving a letter\n",
    "\n",
    "def get_emotion(face_img_gray):\n",
    "    resized = cv2.resize(face_img_gray, (48, 48))\n",
    "    resized = resized.reshape(1, 48, 48, 1) / 255.0\n",
    "    prediction = emotion_model.predict(resized, verbose=0)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "def get_sign_language_char(hand_img_color):\n",
    "    resized = cv2.resize(hand_img_color, (64, 64))\n",
    "    resized = resized.reshape(1, 64, 64, 3) / 255.0\n",
    "    prediction = asl_model.predict(resized, verbose=0)\n",
    "    confidence = np.max(prediction)\n",
    "    label_index = np.argmax(prediction)\n",
    "    if label_index >= len(asl_labels):\n",
    "        return \"\", 0.0\n",
    "    return asl_labels[label_index], confidence\n",
    "\n",
    "def speak_text(text, emotion):\n",
    "    if emotion == \"Sad\":\n",
    "        engine.setProperty(\"rate\", 130)\n",
    "        engine.setProperty(\"volume\", 0.6)\n",
    "    elif emotion == \"Happy\":\n",
    "        engine.setProperty(\"rate\", 180)\n",
    "        engine.setProperty(\"volume\", 1.0)\n",
    "    elif emotion == \"Angry\":\n",
    "        engine.setProperty(\"rate\", 160)\n",
    "        engine.setProperty(\"volume\", 1.0)\n",
    "    else:\n",
    "        engine.setProperty(\"rate\", 150)\n",
    "        engine.setProperty(\"volume\", 0.9)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "print(\"Press 's' to save predicted character to sentence\")\n",
    "print(\"Press 't' to speak the sentence\")\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    hand_region = frame[100:300, 400:600]\n",
    "    face_region = frame[50:250, 50:250]\n",
    "    face_gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if frame_count % predict_interval == 0 and cooldown == 0:\n",
    "        label, conf = get_sign_language_char(hand_region)\n",
    "        live_prediction = label\n",
    "\n",
    "        if label == last_pred:\n",
    "            same_count += 1\n",
    "        else:\n",
    "            same_count = 1\n",
    "        last_pred = label\n",
    "\n",
    "        if same_count >= stable_threshold:\n",
    "            stable_char = label\n",
    "            same_count = 0\n",
    "            print(f\"[Stable Prediction]: {stable_char}\")\n",
    "\n",
    "    frame_count += 1\n",
    "    if cooldown > 0:\n",
    "        cooldown -= 1\n",
    "\n",
    "    # Draw ROIs\n",
    "    cv2.rectangle(frame, (400, 100), (600, 300), (255, 0, 0), 2)\n",
    "    cv2.rectangle(frame, (50, 50), (250, 250), (0, 255, 0), 2)\n",
    "\n",
    "    # Display predictions\n",
    "    cv2.putText(frame, f\"Live Sign: {live_prediction}\", (20, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"Sentence: {sentence}\", (20, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 255, 50), 2)\n",
    "\n",
    "    cv2.imshow(\"Sign Language to Speech\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s') and stable_char != \"\":\n",
    "        if stable_char == \"space\":\n",
    "            sentence += \" \"qq\n",
    "        elif stable_char == \"del\" and len(sentence) > 0:\n",
    "            sentence = sentence[:-1]\n",
    "        elif stable_char not in [\"nothing\", \"space\", \"del\"]:\n",
    "            sentence += stable_char\n",
    "        cooldown = cooldown_frames  # Start cooldown\n",
    "        print(f\"[Added]: {stable_char} -> {sentence}\")\n",
    "\n",
    "    elif key == ord('t'):\n",
    "        emotion = get_emotion(face_gray)\n",
    "        print(f\"[Emotion]: {emotion}\")\n",
    "        speak_text(sentence, emotion)\n",
    "        sentence = \"\"\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54ba18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
